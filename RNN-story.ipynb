{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn_cell\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#weight 매트릭스 초기화 함수\n",
    "#https://github.com/therne/dmn-tensorflow 참고\n",
    "def weight(name, shape, init='he', range=None):\n",
    "    initializer = tf.constant_initializer()\n",
    "    if init == 'xavier':\n",
    "        fan_in, fan_out = _get_dims(shape)\n",
    "        range = math.sqrt(6.0 / (fan_in + fan_out))\n",
    "        initializer = tf.random_uniform_initializer(-range, range)\n",
    "\n",
    "    elif init == 'he':\n",
    "        fan_in, _ = shape\n",
    "        std = math.sqrt(2.0 / fan_in)\n",
    "        initializer = tf.random_normal_initializer(stddev=std)\n",
    "\n",
    "    elif init == 'normal':\n",
    "        initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "    elif init == 'uniform':\n",
    "        if range is None:\n",
    "            raise ValueError(\"range must not be None if uniform init is used.\")\n",
    "        initializer = tf.random_uniform_initializer(-range, range)\n",
    "\n",
    "    var = tf.get_variable(name, shape, initializer=initializer)\n",
    "    tf.add_to_collection('l2', tf.nn.l2_loss(var))  # Add L2 Loss\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##문자열 처리 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#문장 부호 제거\n",
    "def remove_marks(strIn):\n",
    "    marks = ['.', ',', '-', '!', '?', '\\\"']\n",
    "    \n",
    "    result = strIn\n",
    "    for mark in marks:\n",
    "        result = result.replace(mark, '')\n",
    "    \n",
    "    result = result.strip()\n",
    "    return result\n",
    "\n",
    "\n",
    "#데이터 읽어오기\n",
    "def read_story():\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open('stories.txt', 'r') as f:\n",
    "        for line in f: #문장을 한 줄씩 읽어서\n",
    "            chunks = line.split('\\t') #탭을 기준으로 분리. 앞쪽이 문장, 뒤쪽은 클래스 레이블\n",
    "            sentences.append( remove_marks(chunks[0]) ) #문장에서 문장부호 제거\n",
    "            labels.append(int(chunks[1]))\n",
    "            \n",
    "    return sentences, labels\n",
    "\n",
    "#본 예제에서는 GloVe 벡터를 단어 representation로 사용\n",
    "#http://nlp.stanford.edu/projects/glove/ 에서 다운\n",
    "def load_glove(dim):\n",
    "    word2vec = {} \n",
    "    with open( \"glove.6B.\" + str(dim) + \"d.txt\") as f: \n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            word2vec [l[0]] = map( float, l[1:])\n",
    "    return word2vec\n",
    "\n",
    "#GloVe에 미리 정의되지 않은 단어에 대해서는, 랜덤하게 representation 생성\n",
    "def create_vector(word, word2vec, word_vector_size):\n",
    "    vector = np.random.uniform( 0.0, 1.0, (word_vector_size,) )\n",
    "    word2vec[word] = vector\n",
    "    return vector\n",
    "\n",
    "#문장에서 하나의 단어를 입력으로 받아 GloVe vector로 바꾸는 함수\n",
    "def process_word(word, word2vec, vocab, ivocab, word_vector_size):\n",
    "    if not word in word2vec:\n",
    "        create_vector(word, word2vec, word_vector_size)\n",
    "    if not word in vocab:\n",
    "        next_index = len(vocab)\n",
    "        vocab[word] = next_index\n",
    "        ivocab[next_index] = word\n",
    "        \n",
    "    return word2vec[word]\n",
    "\n",
    "#문장을 받아서, 띄어쓰기 단위로 분리하고 GloVe representation으로 바꾸어 전체 입력 데이터를 만드는 함수\n",
    "#각 문장의 단어수도 저장\n",
    "def make_input_vector(sentences, word2vec, vocab, ivocab, word_vector_size):\n",
    "    inputs = []\n",
    "    num_words = []\n",
    "    for line in sentences:\n",
    "        inp = line.lower().split(' ') #띄어쓰기 기준으로 문장 나누기\n",
    "        inp = [ w for w in inp if len(w) > 0] \n",
    "        \n",
    "        inp_vector = [process_word( word = w,\n",
    "                                  word2vec = word2vec,\n",
    "                                  vocab = vocab,\n",
    "                                  ivocab = ivocab,\n",
    "                                  word_vector_size = word_vector_size) for w in inp]\n",
    "        inp_vector = np.vstack((inp_vector))\n",
    "        #print inp_vector.shape\n",
    "        inputs.append( inp_vector )\n",
    "        num_words.append(len(inp))\n",
    "        #print len(vocab), len(ivocab)\n",
    "        \n",
    "    return inputs, num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_glove\n"
     ]
    }
   ],
   "source": [
    "#GloVe representation 의 벡터 사이즈 지정\n",
    "word_vector_size = 100\n",
    "assert word_vector_size in [50, 100, 200, 300]\n",
    "\n",
    "#GloVe 로딩\n",
    "print 'load_glove'\n",
    "word2vec = load_glove(word_vector_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data Padding, 가장 긴 문장을 기준으로 zero padding\n",
    "def process_batch():\n",
    "        start_index = 0\n",
    "        end_index = 200\n",
    "        \n",
    "        inp = inputs[start_index:end_index]\n",
    "        num_word = num_words[start_index:end_index]\n",
    "        label = labels[start_index:end_index]\n",
    "        \n",
    "        B, V = batch_size, word_vector_size\n",
    "\n",
    "        L = 0\n",
    "        for n in range(B):\n",
    "            sent_len = len(inp[n])\n",
    "            L = max(L, sent_len)\n",
    "\n",
    "\n",
    "        new_input = np.zeros([B, L, V])  # zero padding\n",
    "        new_labels = []\n",
    "        new_num_words = []\n",
    "        for n in range(B):\n",
    "            new_input[n, :num_word[n]] = inp[n]\n",
    "            new_labels.append(label[n])\n",
    "            new_num_words.append(num_word[n])\n",
    "\n",
    "\n",
    "        return new_input, new_labels, new_num_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##모델 parameter define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "ivocab = {}\n",
    "\n",
    "batch_size = 200 \n",
    "hidden_size = 50 #GRU hidden layer의 노드 수\n",
    "num_classes = 4\n",
    "learning_rate = 0.001\n",
    "sentences, labels = read_story()\n",
    "inputs, num_words = make_input_vector(sentences, word2vec, vocab, ivocab, word_vector_size)\n",
    "vocab_size = len(vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##RNN 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#B: batch size, L: 가장 긴 문장의 길이(=가장 긴 문장에 있는 단어의 수)\n",
    "#V: GloVe word vector size, h: GRU Hidden Node의 수, A: 데이터에 포함된 단어의 총 갯수\n",
    "B, L = batch_size, max(num_words)\n",
    "V, h, A = word_vector_size, hidden_size, vocab_size\n",
    "\n",
    "inp, len_sentences, label = process_batch()\n",
    "inp = np.float32(inp)\n",
    "#feed_dict={inp:new_input, len_sentences:new_num_words, label:new_labels}\n",
    "\"\"\"\n",
    "inp = tf.placeholder('float32', shape = [B, L, V], name = 'x')\n",
    "len_sentences = tf.placeholder('int32', shape = [B], name = 'x_len')\n",
    "label = tf.placeholder('int32', shape=[B], name = 'y')\n",
    "\"\"\"\n",
    "\n",
    "#GRU Cell 정의\n",
    "gru = rnn_cell.GRUCell(h)\n",
    "outputs, final_states = tf.nn.dynamic_rnn(cell = gru, \n",
    "                                          inputs = inp, \n",
    "                                          sequence_length = len_sentences, \n",
    "                                          dtype = tf.float32)    \n",
    "\n",
    "#GRU hidden 으로부터 클래스를 예측하기 위한 추가 layer\n",
    "with tf.name_scope('answer'):\n",
    "    w_a = weight('answer_weight', [h, num_classes] )\n",
    "    logits = tf.matmul(final_states, w_a)\n",
    "\n",
    "#Cross entropy로 loss function 정의\n",
    "with tf.name_scope('loss'):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, label)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "#정답과 모델이 예측한 답을 비교하여 정확도 측정\n",
    "with tf.name_scope('acc'):\n",
    "    predicts = tf.cast(tf.argmax(logits, 1), 'int32')\n",
    "    corrects = tf.equal(predicts, labels[:batch_size])\n",
    "    num_corrects = tf.reduce_sum(tf.cast(corrects, tf.float32))\n",
    "    accuracy = tf.reduce_mean(tf.cast(corrects, tf.float32))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "opt_op = optimizer.minimize(loss) #, global_step = self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Computational Graph Launching, 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "\n",
    "    for i in range(10):\n",
    "        sess.run(opt_op)\n",
    "        a = sess.run(final_states)\n",
    "        acc_print = sess.run(accuracy)\n",
    "        loss_print = sess.run(loss)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
