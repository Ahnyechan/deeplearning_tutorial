{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learing\n",
    "\n",
    "Deep Reinforcement Learning이란, 기존의 강화학습에서의 Q function을 딥러닝으로 근사하는 모델을 의미한다. 대표적으로 구글 Deep Mind의 Atari와 AlphaGo 역시 이 Deep Reinforcement Learning 응용의 한가지이다.\n",
    "\n",
    "이번 파트에서는 Deep Reinforcement Learning을 이용해서 간단한 2차원 게임을 플레이하고, reward로 부터 스스로 학습하는 Kaparthy의 오픈소스 예제를 실습해 본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img alt=\"Deep RL\" style=\"border-width:0\" width=\"600\" src=\"images\\example.gif?raw=true\" />\n",
    "\n",
    "\n",
    "(출처: https://github.com/nivwusquorum/tensorflow-deepq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Reinforcement Learning\n",
    "\n",
    "Reinforcement Learning, 이하 RL은 supervised learning과 달리 데이터에 대한 정확한 정답을 받지 않고, 내가 한 행동에 대한 reward feedback 만으로 학습을 수행하는 알고리즘이다. 이를 강화학습이라 부르며, 이것을 수행하는 가장 대표적인 알고리즘으로 Q-Learning 이 있다.\n",
    "\n",
    "RL을 이해하는 것은 매우 많은 공부를 필요로 하기 때문에, 우선 2D게임을 예로 들어 아주 기본적인 개념만 살펴본다.\n",
    "\n",
    "* **State**: 게임에서의 각 물체들의 위치, 속도, 벽과의 거리 등을 의미한다.\n",
    "* **Action**: 게임을 플레이하는 주인공의 행동을 의미한다. 여기서는 4가지 방향에 대한 움직임이 이에 해당한다.\n",
    "* **Reward**: 게임을 플레이하면서 받는 score. 여기서는 초록색을 먹으면 +1 , 빨간색을 먹으면 -1을 점수로 받는다.\n",
    "* **Value**: 해당 action 또는 state가 미래에 얼마나 큰 reward를 가져올 지에 대한 기댓값.\n",
    "* **Policy**: 주인공이 현재의 게임 State에서 Reward를 최대한 얻기 위해 Action을 선택하는 전략. 한마디로 [게임을 플레이하는 방법]을 의미한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Interaction(or Perception-Action Cycle) in Reinforcement Learning\n",
    "<img src='images\\rl.png' width=600>\n",
    "(출처 : Sutton, 1998, Reinforcement Learning: An Introduction)\n",
    "\n",
    "강화학습 학습이 다른 머신러닝 방법(supervised learning, unsupervised learning)과 다른 가장 큰 특징 중 하나는, 바로 환경과의 실시간 인터렉션을 가정한 학습 모델이라는 점이다. 강화학습은 생물체의 학습과정을 수학적으로 모델링하여 구현된 알고리즘이기 때문에 다른 방법들에 비해 보다 사람 혹은 동물의 학습과 유사하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid World\n",
    "<img src='images\\gridworld2.png'>\n",
    "\n",
    "<br>(출처 : Sutton, 1998, Reinforcement Learning: An Introduction)\n",
    "<br>State : 5x5\n",
    "<br>Action : 4방향이동\n",
    "<br>Reward : A에 도착하면 +10, B에 도착하면 +5, 벽에 부딪히면 -1, 그이외 0\n",
    "<br>Discounted Factor : 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discounted Factor\n",
    "<img src='images\\discounted_factor.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q table\n",
    "<img src='images\\q_table.png'>\n",
    "<br>(출처 : http://gruposagama.com/pages/q-learning.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning Algorithm\n",
    "<img src='images\\qlearning.png'>\n",
    "\n",
    "<img src='images\\q_learning3.jpg' width=800>\n",
    "\n",
    "<br>(출처 : http://www.randomant.net/wp-content/uploads/2016/05/q_learning3.jpg)\n",
    "<br>(출처 : http://people.revoledu.com/kardi/tutorial/ReinforcementLearning/Q-Learning-Example.htm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Reinforcement Learning : Q function  -> Deep Learninig\n",
    "<img src='images\\deeprl.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Deep Reinforcement Learning\n",
    "<br>\n",
    "아래의 예제 코드를 실행시키기 위해서는 리눅스 shell에서 다음의 명령어를 실행해 필요한 python package를 설치해야한다.\n",
    "####### pip install future euclid redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tf_rl.controller import DiscreteDeepQ, HumanController\n",
    "from tf_rl.simulation import KarpathyGame\n",
    "from tf_rl import simulate\n",
    "from tf_rl.models import MLP\n",
    "\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment Settings\n",
    "\n",
    "이제 우리가 원하는 게임 환경을 설정하고, 적절한 reward와 object의 개수 및 observation 을 조절한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_settings = {\n",
    "    'objects': [\n",
    "        'friend',\n",
    "        'enemy',\n",
    "    ],\n",
    "    'colors': {\n",
    "        'hero':   'yellow',\n",
    "        'friend': 'green',\n",
    "        'enemy':  'red',\n",
    "    },\n",
    "    'object_reward': {\n",
    "        'friend': 1,\n",
    "        'enemy': -1,\n",
    "    },\n",
    "    \"num_objects\": {\n",
    "        'friend' : 25,\n",
    "        'enemy' :  25,\n",
    "    },\n",
    "    \n",
    "    'hero_bounces_off_walls': False,\n",
    "    'world_size': (700,500),\n",
    "    'hero_initial_position': [400, 300],\n",
    "    'hero_initial_speed':    [0,   0],\n",
    "    \"maximum_speed\":         [50, 50],\n",
    "    \"object_radius\": 10.0,\n",
    "    \"num_observation_lines\" : 32, # the number of antennas\n",
    "    \"observation_line_length\": 240., # the length of antennas\n",
    "    \"tolerable_distance_to_wall\": 50, \n",
    "    \"wall_distance_penalty\":  -0.0, # if the hero is close to wall, that receives penalty\n",
    "    \"delta_v\": 50 # speed value\n",
    "}\n",
    "\n",
    "# create the game simulator\n",
    "g = KarpathyGame(current_settings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Architecture\n",
    "\n",
    "이제 Q function을 근사하기 위한 딥러닝 모델을 만들어보자. 이번 예제에서는 위에서 보았던 4층짜리 MLP를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True)))\n",
    "\n",
    "    \n",
    "\n",
    "journalist = tf.train.SummaryWriter(\"/tmp/drl\")\n",
    "\n",
    "# Brain maps from observation to Q values for different actions.\n",
    "# Here it is a done using a multi layer perceptron with 2 hidden layers\n",
    "brain = MLP([g.observation_size,], [200, 200, g.num_actions],  [tf.tanh, tf.tanh, tf.identity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make an Agent\n",
    "\n",
    "이제 Discrete Deep Q learning 알고리즘이 이 게임을 플레이하면서 학습을 하도록 agent로 설정을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The optimizer to use. Here we use RMSProp as recommended by the publication\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate= 0.001, decay=0.9)\n",
    "\n",
    "# DiscreteDeepQ object\n",
    "current_controller = DiscreteDeepQ(g.observation_size, g.num_actions, brain, optimizer, session,\n",
    "                                   discount_rate=0.99, exploration_period=5000, max_experience=10000, \n",
    "                                   store_every_nth=4, train_every_nth=4,\n",
    "                                   summary_writer=journalist)\n",
    "\n",
    "session.run(tf.initialize_all_variables())\n",
    "session.run(current_controller.target_network_update)\n",
    "#journalist.add_graph(session.graph_def)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Play the Game\n",
    "\n",
    "실제로 게임을 플레이하면서 강화학습이 일어나는 과정을 지켜보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<?xml version=\"1.0\"?>\n",
       "\n",
       "<svg height=\"600\" width=\"720\" >\n",
       "\n",
       " <g style=\"fill-opacity:1.0; stroke:black;\n",
       "\n",
       "  stroke-width:1;\">\n",
       "\n",
       "  <rect x=\"10\" y=\"10\" height=\"500\"\n",
       "\n",
       "        width=\"700\" style=\"fill:none;\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"292\" y2=\"191\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"288\" y2=\"238\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"274\" y2=\"283\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"252\" y2=\"325\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"222\" y2=\"361\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"186\" y2=\"391\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"144\" y2=\"413\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"99\" y2=\"427\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"52\" y2=\"431\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"5\" y2=\"427\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-39\" y2=\"413\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-80\" y2=\"391\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-116\" y2=\"361\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-146\" y2=\"325\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-169\" y2=\"283\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-182\" y2=\"238\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-187\" y2=\"191\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-182\" y2=\"145\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-169\" y2=\"99\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-146\" y2=\"58\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-116\" y2=\"22\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-80\" y2=\"-7\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"-39\" y2=\"-29\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"5\" y2=\"-43\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"52\" y2=\"-48\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"99\" y2=\"-43\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"144\" y2=\"-29\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"186\" y2=\"-7\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"222\" y2=\"22\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"252\" y2=\"58\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"274\" y2=\"99\" />\n",
       "\n",
       "  <line x1=\"52\" y1=\"191\" x2=\"288\" y2=\"145\" />\n",
       "\n",
       "  <circle cx=\"478\" cy=\"171\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"457\" cy=\"52\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"292\" cy=\"391\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"232\" cy=\"454\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"293\" cy=\"343\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"213\" cy=\"499\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"402\" cy=\"126\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"390\" cy=\"392\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"450\" cy=\"66\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"214\" cy=\"367\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"646\" cy=\"121\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"268\" cy=\"406\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"259\" cy=\"415\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"181\" cy=\"281\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"599\" cy=\"52\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"124\" cy=\"66\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"535\" cy=\"188\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"456\" cy=\"130\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"199\" cy=\"428\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"529\" cy=\"115\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"332\" cy=\"132\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"28\" cy=\"266\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"476\" cy=\"96\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"657\" cy=\"152\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"284\" cy=\"42\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"586\" cy=\"249\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"53\" cy=\"65\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"598\" cy=\"340\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"362\" cy=\"334\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"288\" cy=\"86\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"136\" cy=\"105\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"125\" cy=\"283\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"323\" cy=\"32\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"525\" cy=\"129\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"674\" cy=\"276\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"111\" cy=\"377\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"682\" cy=\"121\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"468\" cy=\"492\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"22\" cy=\"359\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"155\" cy=\"490\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"174\" cy=\"163\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"676\" cy=\"440\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"566\" cy=\"186\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"600\" cy=\"365\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"167\" cy=\"109\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"633\" cy=\"434\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"328\" cy=\"425\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"148\" cy=\"66\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"600\" cy=\"414\" r=\"10\"\n",
       "\n",
       "          style=\"fill:red;\" />\n",
       "\n",
       "  <circle cx=\"91\" cy=\"331\" r=\"10\"\n",
       "\n",
       "          style=\"fill:green;\" />\n",
       "\n",
       "  <circle cx=\"52\" cy=\"191\" r=\"10\"\n",
       "\n",
       "          style=\"fill:yellow;\" />\n",
       "\n",
       "  <text x=\"10\" y=\"535\" font-size=\"15\">\n",
       "\n",
       "   fps = 107.0\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"555\" font-size=\"15\">\n",
       "\n",
       "   nearest wall = 32.7\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"575\" font-size=\"15\">\n",
       "\n",
       "   reward       = -0.0\n",
       "\n",
       "  </text>\n",
       "\n",
       "  <text x=\"10\" y=\"595\" font-size=\"15\">\n",
       "\n",
       "   objects eaten => enemy: 2, friend: 3\n",
       "\n",
       "  </text>\n",
       "\n",
       " </g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<tf_rl.utils.svg.Scene instance at 0x7f344873dc20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FPS          = 30\n",
    "ACTION_EVERY = 3\n",
    "    \n",
    "fast_mode = True\n",
    "if fast_mode:\n",
    "    WAIT, VISUALIZE_EVERY = False, 20\n",
    "else:\n",
    "    WAIT, VISUALIZE_EVERY = True, 1\n",
    "\n",
    "    \n",
    "try:\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        simulate(simulation=g,\n",
    "                 controller=current_controller,\n",
    "                 fps=FPS,\n",
    "                 visualize_every=VISUALIZE_EVERY,\n",
    "                 action_every=ACTION_EVERY,\n",
    "                 wait=WAIT,\n",
    "                 disable_training=False,\n",
    "                 simulation_resolution=0.001,\n",
    "                 save_path=None)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Interrupted\")\n",
    "    \n",
    "\n",
    "#session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applications\n",
    "* Parameter 들을 바꾸어 enemy와 friend의 개수 차이가 최대한 많이 나도록 agent를 학습시켜본다.\n",
    "* Boss object를 추가해본다.\n",
    "* Deep RL에서의 tensorboard를 열어서 visualize를 해본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Materials\n",
    "\n",
    "<br>https://github.com/aikorea/awesome-rl\n",
    "<br>https://openai.com/blog/universe/\n",
    "<br>http://alpha.openai.com/miniwob/index.html\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
